# NanoNeuron

> 7 fun√ß√µes simples do JavaScript que far√£o voc√™ ter uma ideia de como as m√°quinas podem "aprender"  literalmente.

_Em outros idiomas: [–†—É—Å—Å–∫–∏–π](README.ru-RU.md), [English](README.md)_

> _Voc√™ tamb√©m pode se interessar por ü§ñ [Experimentos interativos de Machine Learning (em ingl√™s)](https://github.com/trekhleb/machine-learning-experiments)_

## Resumo

[NanoNeuron](https://github.com/trekhleb/nano-neuron) √© uma vers√£o _bem simples_ do conceito de Neur√¥nio em uma Rede Neural. NanoNeuron √© treinado para converter valores de graus Celsius em Fahrenheit.

O c√≥digo de exemplo [NanoNeuron.js](https://github.com/trekhleb/nano-neuron/blob/master/NanoNeuron.js) cont√©m 7 simples fun√ß√µes JavaScript (sobre predi√ß√£o de modelo, c√°lculo de custo, propaga√ß√£o e retropropaga√ß√£o, e treinamento) que ir√° te dar a vis√£o de como as m√°quinas podem literalmente "aprender". Sem bibliotecas de terceiros, sem conjuntos de dados externos ou depend√™ncias, apenas simples e puramente fun√ß√µes JavaScript.

‚òùüèªEssas fun√ß√µes **N√ÉO** s√£o, de nenhuma forma, um guia completo para aprendizado de m√°quina (_"machine learning" em ingl√™s_). Um monte de conceitos de machine learning foram desconsiderados e muito simplificados! Essa simplifica√ß√£o foi feita com o prop√≥sito de dar ao leitor apenas um entendimento **b√°sico** da vis√£o de como as m√°quinas podem aprender e por fim para tornar poss√≠vel para o leitor reconhecer que isso n√£o √© um "aprendizado M√ÅGICO de m√°quina" mas sim um "aprendizado MATEM√ÅTICO de m√°quina" ü§ì.

## O que o nosso NanoNeuron ir√° aprender

Provavelmente voc√™ j√° ouviu falar sobre Neur√¥nios no contexto de [Redes Neurais](https://pt.wikipedia.org/wiki/Rede_neural_artificial). NanoNeuron √© isso mas de forma simples e vamos implementar desde o in√≠cio. Para efeitos de simplicidade n√≥s n√£o iremos construir uma rede de NanoNeuron. Teremos tudo funcionando no mesmo lugar, fazendo algumas predi√ß√µes m√°gicas para n√≥s. S√≥ pra voc√™ saber, vamos ensinar esse NanoNeuron a converter (predizer) a temperatura em graus Celsius para Fahrenheit.

A prop√≥sito, a f√≥rmula para converter graus Celsius em Fahrenheit √© essa:

![Celsius para Fahrenheit](https://github.com/trekhleb/nano-neuron/blob/master/assets/01_celsius_to_fahrenheit.png?raw=true)

Mas por enquanto nosso NanoNeuron n√£o sabe disso...

### O modelo NanoNeuron

Vamos implementar nossa fun√ß√£o de modelo do NanoNeuron. Ela implementa uma depend√™ncia linear b√°sica entre `x` e `y` que se parece com `y = w * x + b`. Basicamente, nosso NanoNeuron √© uma "crian√ßa" na "escola" aprendendo a desenhar uma linha reta nas coordenadas `XY`.

Vari√°veis `w`, `b` s√£o par√¢metros do modelo. NanoNeuron s√≥ conhece esses dois par√¢metros da fun√ß√£o linear. Eles s√£o algo que NanoNeuron ir√° "aprender" durante o processo de treinamento.

A √∫nica coisa que o NanoNeuron pode fazer √© imitar a depend√™ncia linear. No m√©todo `predict()` √© aceito um dado de entrada `x` e prediz a sa√≠da `y`. Nenhuma m√°gica aqui.

```javascript
function NanoNeuron(w, b) {
  this.w = w;
  this.b = b;
  this.predict = (x) => {
    return x * this.w + this.b;
  }
}
```

_(...espera... [regress√£o linear](https://pt.wikipedia.org/wiki/Regress%C3%A3o_linear) √© voc√™?)_  üßê

### Convers√£o de graus Celsius para Fahrenheit

A temperatura em graus Celsius pode ser convertida para Fahrenheit usando a seguinte f√≥rmula: `f = 1.8 * c + 32`, onde `c` √© a temperatura em graus Celsius e `f` a temperatura calculada em Fahrenheit.

```javascript
function celsiusToFahrenheit(c) {
  const w = 1.8;
  const b = 32;
  const f = c * w + b;
  return f;
};
```

Queremos que o nosso NanoNeuron imite essa fun√ß√£o (para aprender que `w = 1.8` e `b = 32`) sem conhecer esses par√¢metros antecipadamente.

Assim √© como a fun√ß√£o de convers√£o de graus Celsius para Fahrenheit ir√° parecer:

![convers√£o de graus Celsius para Fahrenheit](https://github.com/trekhleb/nano-neuron/blob/master/assets/07-converter.png?raw=true)

### Gerando os conjuntos de dados

Antes do treinamento n√≥s precisamos **treinar** e **testar os dados** baseando-se na fun√ß√£o `celsiusToFahrenheit()`. Os conjuntos de dados consistem em pares de valores de entrada e valores de sa√≠da corretamente calculados.

> Na vida real, na maioria dos casos, esses dados s√£o coletados ao inv√©s de gerados. Por exemplo, podemos ter um conjunto de imagens de n√∫meros desenhados √† m√£o e o conjunto com os n√∫meros que explicam qual √© o n√∫mero escrito em cada imagem.

Usaremos os dados de exemplo de TREINAMENTO para treinar nosso NanoNeuron. Antes dele crescer e ser capaz de fazer decis√µes sozinho, precisamos ensin√°-lo o que √© certo e o que √© errado usando os exemplos de treinamento.

Usaremos os exemplos de TESTE para avaliar o quanto nosso NanoNeuron performa bem nos dados que ele nunca viu durante o treinamento. Esse √© o ponto onde podemos ver que a nossa "crian√ßa" cresceu e pode tomar decis√µes sozinha.

```javascript
function generateDataSets() {
  // xTrain -> [0, 1, 2, ...],
  // yTrain -> [32, 33.8, 35.6, ...]
  const xTrain = [];
  const yTrain = [];
  for (let x = 0; x < 100; x += 1) {
    const y = celsiusToFahrenheit(x);
    xTrain.push(x);
    yTrain.push(y);
  }

  // xTest -> [0.5, 1.5, 2.5, ...]
  // yTest -> [32.9, 34.7, 36.5, ...]
  const xTest = [];
  const yTest = [];
  // Ao come√ßar com 0,5 e usar o mesmo incremento de 1 como usamos para o conjunto
  // de treinamento, temos certeza que teremos dados diferentes para comparar.
  for (let x = 0.5; x < 100; x += 1) {
    const y = celsiusToFahrenheit(x);
    xTest.push(x);
    yTest.push(y);
  }

  return [xTrain, yTrain, xTest, yTest];
}
```

### O custo (do erro) da predi√ß√£o

Precisamos ter alguma m√©trica que nos mostre o qu√£o perto nosso modelo de predi√ß√£o est√° dos valores corretos. O c√°lculo do custo (o engano) entre o valor correto calculado de `y` e a `prediction`, que o nosso NanoNeuron criou, ser√° feito usando a seguinte f√≥rmula:

![Custo de predi√ß√£o](https://github.com/trekhleb/nano-neuron/blob/master/assets/02_cost_function.png?raw=true)

Esse √© uma simples diferen√ßa entre dois valores. O quanto mais perto os valores est√£o um do outro, menor a diferen√ßa. Estamos usando uma pot√™ncia de `2` aqui apenas para se livrar dos n√∫meros negativos de forma que `(1 - 2) ^ 2` ser√° o mesmo que `(2 - 1) ^ 2`. Divis√£o por `2` acontece apenas para simplificar depois a f√≥rmula de retropropaga√ß√£o (veja abaixo).

A fun√ß√£o de custo nesse caso, ser√° t√£o simples quanto:

```javascript
function predictionCost(y, prediction) {
  return (y - prediction) ** 2 / 2; // ex.: -> 235.6
}
```

### Propaga√ß√£o (para frente)

Propaga√ß√£o _("forward propagation" em ingl√™s)_ significa fazer uma predi√ß√£o de todos os exemplos de treinamento para os conjuntos de dados `xTrain` e `yTrain` e para calcular o custo m√©dio dessas predi√ß√µes no meio do caminho.

Vamos apenas deixar nosso NanoNeuron dizer sua opini√£o nesse momento, permitindo-o adivinhar como converter a temperatura. Ele deve estar estupidamente errado nessa fase. O custo m√©dio nos mostrar√° o qu√£o errado nosso modelo est√° agora. Esse valor de custo √© realmente importante visto que alterando os par√¢metros NanoNeuron `w` e `b` e fazendo a propaga√ß√£o novamente, estaremos aptos a avaliar depois se nosso NanoNeuron se tornou esperto ou n√£o conforme os par√¢metros mudam.

O custo m√©dio ser√° calculado usando a seguinte f√≥rmula:

![Custo m√©dio](https://github.com/trekhleb/nano-neuron/blob/master/assets/03_average_cost_function.png?raw=true)

Onde `m` √© o n√∫mero de exemplos de treinamento (no nosso caso: `100`).

Esta √© a forma como devemos implementar no c√≥digo:

```javascript
function forwardPropagation(model, xTrain, yTrain) {
  const m = xTrain.length;
  const predictions = [];
  let cost = 0;
  for (let i = 0; i < m; i += 1) {
    const prediction = nanoNeuron.predict(xTrain[i]);
    cost += predictionCost(yTrain[i], prediction);
    predictions.push(prediction);
  }
  // Estamos interessados no custo m√©dio
  cost /= m;
  return [predictions, cost];
}
```

### retropropaga√ß√£o (para tr√°s)

Quando conhecemos o qu√£o certo ou errado nossas predi√ß√µes do NanoNeuron est√£o (baseado no custo m√©dio a este ponto) o que devemos fazer para tornar essas predi√ß√µes mais precisas?

A retropropaga√ß√£o nos d√° a resposta para essa quest√£o. retropropaga√ß√£o _(Backward propagation em ingl√™s)_ √© o processo de avaliar o custo da predi√ß√£o e ajustar os par√¢metros do NanoNeuron `w` e `b` para que as pr√≥ximas e futuras predi√ß√µes sejam mais precisas.

Isso √© onde o aprendizado de m√°quina se parece com m√°gica üßû‚Äç‚ôÇÔ∏è. O conceito chave aqui √© a **derivada** que nos mostra qual passo dar para chegar perto do custo m√≠nimo da fun√ß√£o.

Lembre-se, encontrar o custo m√≠nimo da fun√ß√£o √© o objetivo final do processo de treinamento. Se encontrarmos ambos valores de `w` e `b` de forma que o custo m√©dio da nossa fun√ß√£o seja pequeno, isso significa que o modelo NanoNeuron fez predi√ß√µes √≥timas e precisas.

Derivada √© um grande e separado t√≥pico que n√£o iremos cobrir neste artigo. [Wikipedia](https://pt.wikipedia.org/wiki/Derivada) pode te ajudar a entender melhor sobre isso.

Uma coisa sobre as derivadas que ir√° te ajudar a entender como a retropropaga√ß√£o funciona √© que a derivada √© ela representa a inclina√ß√£o da reta tangente ao gr√°fico desta fun√ß√£o em um determinado ponto.

![Inclina√ß√£o da derivada](https://www.mathsisfun.com/calculus/images/slope-x2-2.svg)

_Origem da imagem: [MathIsFun](https://www.mathsisfun.com/calculus/derivatives-introduction.html)_

Por exemplo, no gr√°fico acima, voc√™ pode ver que se estivermos no ponto `(x=2, y=4)` ent√£o a inclina√ß√£o nos diz para ir para a `esquerda` e para `baixo` para obter a fun√ß√£o m√≠nima. Note tamb√©m que quanto maior a inclina√ß√£o, mais r√°pido nos movemos para o m√≠nimo.

As derivadas da nossa fun√ß√£o `averageCost` _(custo m√©dio em ingl√™s)_ para os par√¢metros `w` e `b` se parecem com:

![dW](https://github.com/trekhleb/nano-neuron/blob/master/assets/04_dw.png?raw=true)

![dB](https://github.com/trekhleb/nano-neuron/blob/master/assets/04_db.png?raw=true)

Onde `m` √© o n√∫mero de exemplos de treinamento (no nosso caso: `100`).

_Voc√™ pode aprender mais sobre as regras das derivadas e como obter uma derivada de fun√ß√µes complexas [aqui](https://brasilescola.uol.com.br/matematica/introducao-ao-estudo-das-derivadas.htm) ou na [indica√ß√£o do autor original (em ingl√™s)](https://www.mathsisfun.com/calculus/derivatives-rules.html)._

```javascript
function backwardPropagation(predictions, xTrain, yTrain) {
  const m = xTrain.length;
  // No come√ßo n√£o conhecemos de que forma nossos par√¢metros 'w' e 'b' precisam ser alterados.
  // Portanto vamos configurar cada par√¢metro para 0.
  let dW = 0;
  let dB = 0;
  for (let i = 0; i < m; i += 1) {
    dW += (yTrain[i] - predictions[i]) * xTrain[i];
    dB += yTrain[i] - predictions[i];
  }
  // Estamos interessados em deltas m√©dios de cada par√¢metro.
  dW /= m;
  dB /= m;
  return [dW, dB];
}
```

### Treinando o modelo

Agora que sabemos como avaliar a exatid√£o do nosso modelo para todo o conjunto de exemplos (_propaga√ß√£o_), n√≥s precisamos tamb√©m saber como fazer pequenos ajustes nos par√¢metros `w` e `b` do nosso modelo (_retropropaga√ß√£o_). Mas o problema √© que se rodarmos apenas uma vez a propaga√ß√£o e a retropropaga√ß√£o, n√£o ser√° o suficiente para o nosso modelo aprender qualquer lei/tend√™ncia dos dados de treinamento. Voc√™ deve comparar isso com um dia da escola prim√°ria para a crian√ßa. Ela deve ir para a escola n√£o apenas uma vez, mas dia ap√≥s dia e ano ap√≥s ano para aprender algo.

Ent√£o precisamos repetir as propaga√ß√µes do nosso modelo v√°rias vezes. Isto √© exatamente o que a fun√ß√£o `trainModel()` faz. √â como um "professor" para nosso modelo do NanoNeuron:

- ela ir√° passar um tempo (`epochs`) com o nosso ligeiro modelo do NanoNeuron e tentar√° trein√°-lo/ensin√°-lo,
- usar√° "livros" espec√≠ficos (os conjuntos de dados `xTrain` e `yTrain`) para treinar,
- ir√° for√ßar nossa crian√ßa a aprender pesado (r√°pido) usando um par√¢metro de ajuste `alpha`.

Uma nota sobre a taxa de aprendizado `alpha`. Ela √© simplesmente um multiplicador dos valores de `dW` e `dB` que calculamos durante a retropropaga√ß√£o. Assim, as derivadas nos apontam para a dire√ß√£o que precisamos para obter a fun√ß√£o de custo m√≠nimo (indicadores `dW` e `dB`) e isso nos mostra tamb√©m o qu√£o r√°pido precisamos ir naquela dire√ß√£o  (valores absolutos de `dW` e `dB`). Ent√£o precisamos multiplicar o tamanho dos passos de `alpha` para ajustar nosso movimento ao m√≠nimo, mais r√°pido ou mais devagar. Algumas vezes se usarmos um valor alto para `alpha`, vamos simplesmente passar do m√≠nimo e nunca vamos encontr√°-lo.

A analogia com o professor pode ser que quanto mais ele for√ßa nossa "crian√ßa nano" a ser mais r√°pida, ela ir√° aprender, mas se for√ßarmos demais, a "crian√ßa" ter√° um ataque de nervos e n√£o ser√° capaz de aprender nada ü§Ø.

Aqui √© como vamos fazer para atualizar nossos par√¢metros `w` e `b` do modelo:

![w](https://github.com/trekhleb/nano-neuron/blob/master/assets/05_w.png?raw=true)

![b](https://github.com/trekhleb/nano-neuron/blob/master/assets/05_b.png?raw=true)

E aqui est√° nossa fun√ß√£o de treinamento:

```javascript
function trainModel({model, epochs, alpha, xTrain, yTrain}) {
  // Esse √© o hist√≥rico de aprendizado do NanoNeuron.
  const costHistory = [];

  // Vamos come√ßar enumerando as √©pocas
  for (let epoch = 0; epoch < epochs; epoch += 1) {
    // Propaga√ß√£o
    const [predictions, cost] = forwardPropagation(model, xTrain, yTrain);
    costHistory.push(cost);
  
    // retropropaga√ß√£o
    const [dW, dB] = backwardPropagation(predictions, xTrain, yTrain);
  
    // Ajustar os par√¢metros do nosso NanoNeuron para aumentar a acur√°cia do nosso modelo de predi√ß√µes.
    nanoNeuron.w += alpha * dW;
    nanoNeuron.b += alpha * dB;
  }

  return costHistory;
}
```

### Juntando as pe√ßas

Agora vamos usar as fun√ß√µes que criamos acima.

Vamos criar nossa inst√¢ncia do modelo do NanoNeuron. Nesse momento o NanoNeuron n√£o sabe que valores deve usar nos par√¢metros `w` e `b`. Ent√£o vamos colocar um valor qualquer em `w` e `b`.

```javascript
const w = Math.random(); // ex: -> 0.9492
const b = Math.random(); // ex: -> 0.4570
const nanoNeuron = new NanoNeuron(w, b);
```

Gerar os conjuntos de dados do treinamento e o de testes.

```javascript
const [xTrain, yTrain, xTest, yTest] = generateDataSets();
```

Vamos treinar nosso modelo com um pequeno incremento (`0,0005`) por passo para `70.000` √©pocas. Voc√™ pode brincar com esses par√¢metros, eles foram definidos empiricamente.

```javascript
const epochs = 70000;
const alpha = 0.0005;
const trainingCostHistory = trainModel({model: nanoNeuron, epochs, alpha, xTrain, yTrain});
```

Vamos checar o quanto a fun√ß√£o de custo mudou durante o treinamento. Esperamos que o custo ap√≥s o treinamento seja menor que antes. Isso significa que o NanoNeuron se tornou esperto. O oposto tamb√©m √© poss√≠vel.

```javascript
console.log('Custo antes do treinamento:', trainingCostHistory[0]); // ex: -> 4694.3335043
console.log('Custo depois do treinamento:', trainingCostHistory[epochs - 1]); // ex: -> 0.0000024
```

Isso √© como o custo do treinamento muda atrav√©s das √©pocas. No eixo `x` √© a √©poca multiplicada por 1000.

![Processo de treinamento](https://github.com/trekhleb/nano-neuron/blob/master/assets/06-training-process.png?raw=true)

Vamos dar uma olhada nos par√¢metros do NanoNeuron para ver o que ele aprendeu. Esperamos que os par√¢metros `w` e `b` do NanoNeuron sejam similares com os que temos na fun√ß√£o `celsiusToFahrenheit()` (`w = 1.8` e `b = 32`) visto que treinamos o NanoNeuron para imitar isso.

```javascript
console.log('Par√¢metros NanoNeuron:', {w: nanoNeuron.w, b: nanoNeuron.b}); // ex: -> {w: 1.8, b: 31.99}
```

Avalie a acur√°cia do modelo usando os dados de teste para ver o quanto o NanoNeuron se d√° bem com predi√ß√µes de dados desconhecidos. √â esperado que os custos das predi√ß√µes no conjunto de testes seja pr√≥ximo do custo de treinamento. Isso pode significar que nosso NanoNeuron performa bem em dados que ele conhece e os que ele n√£o conhece.

```javascript
[testPredictions, testCost] = forwardPropagation(nanoNeuron, xTest, yTest);
console.log('Custo com novos dados de teste:', testCost); // ex: -> 0.0000023
```

Agora, visto que nossa "crian√ßa" NanoNeuron performou bem na "escola" durante o treinamento e ele pode converter graus Celsius em Fahrenheit corretamente, mesmo para dados que nunca viu, podemos cham√°-lo de "esperto" e pergunt√°-lo algumas coisas. Esse era o objetivo final de todo nosso processo de treinamento.

```javascript
const tempInCelsius = 70;
const customPrediction = nanoNeuron.predict(tempInCelsius);
console.log(`NanoNeuron "acha" que ${tempInCelsius}¬∞C em Fahrenheit √©:`, customPrediction); // -> 158.0002
console.log('Resposta correta √©:', celsiusToFahrenheit(tempInCelsius)); // -> 158
```

Muito pr√≥ximo! Para n√≥s humanos, nosso NanoNeuron √© bom, mas n√£o ideal :)

Bom aprendizado para voc√™!

## Como executar o NanoNeuron

Voc√™ pode clonar esse reposit√≥rio e execut√°-lo localmente:

```bash
git clone https://github.com/trekhleb/nano-neuron.git
cd nano-neuron
```

```bash
node ./NanoNeuron.js
```

## Conceitos desconsiderados do aprendizado de m√°quina

Os seguintes conceitos de _machine learning_ foram pulados e simplificados para uma explica√ß√£o mais simples.

### Divis√£o do conjunto de dados de treinamento/teste

Normalmente voc√™ tem um grande conjunto de dados. Dependendo do n√∫mero de exemplos no conjunto, voc√™ pode querer dividi-lo em 70/30 para treino/teste. Os dados no conjunto devem ser embaralhados aleatoriamente antes da divis√£o. Se o n√∫mero de exemplos √© grande (ex: milh√µes) ent√£o a divis√£o acontece em propor√ß√µes pr√≥ximas a 90/10 ou 95/5 para treino/teste.

### A rede traz o poder

Normalmente voc√™ n√£o observa o uso de apenas um neur√¥nio independente. O poder est√° na [rede neural](https://pt.wikipedia.org/wiki/Rede_neural_artificial) desses neur√¥nios. A rede pode aprender coisas muito mais complexas. NanoNeuron sozinho se parece mais com uma simples [regress√£o linear](https://pt.wikipedia.org/wiki/Regress%C3%A3o_linear) do que uma rede neural.

### Normaliza√ß√£o dos dados de entrada

Antes do treinamento, seria melhor [normalizar os dados de entrada (em ingl√™s)](https://www.jeremyjordan.me/batch-normalization/).

### Implementa√ß√£o vetorizada

Para redes neurais, c√°lculos vetorizados (matriz) trabalham muito mais r√°pido do que la√ßos `for`. Normalmente as propaga√ß√µes (frente e tr√°s) trabalham muito r√°pido se implementadas de forma vetorizada e calculadas usando, por exemplo uma biblioteca Python [Numpy](https://numpy.org/).

### Fun√ß√£o de custo m√≠nimo

A fun√ß√£o de custo que estamos usando nesse exemplo √© muito simplificada. Deveria ter [componentes logar√≠tmicos (em ingl√™s)](https://stackoverflow.com/questions/32986123/why-the-cost-function-of-logistic-regression-has-a-logarithmic-expression/32998675). Alterando a fun√ß√£o de custo tamb√©m ir√° alterar suas derivadas ent√£o o passo de retropropaga√ß√£o tamb√©m deveria usar f√≥rmulas diferentes.

### Fun√ß√£o de ativa√ß√£o

Normalmente a sa√≠da do neur√¥nio deveria passar por uma fun√ß√£o de ativa√ß√£o como a [Sigmoid](https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_sigmoide) ou a [ReLU](https://pt.qwe.wiki/wiki/Rectifier_(neural_networks)) ou outras.
